{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import warnings\n",
    "import random\n",
    "import time\n",
    "\n",
    "from vizdoom import *\n",
    "\n",
    "import numpy as np\n",
    "from skimage import transform, util, color\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MODEL HYPERPARAMETERS### \n",
    "im_size = [200,200,4]        # 200x200 image x 4 frames per input\n",
    "action_size = 3              # 3 possible actions: left, right, shoot\n",
    "learning_rate =  0.0002      # Alpha (aka learning rate)\n",
    "\n",
    "### TRAINING HYPERPARAMETERS\n",
    "total_episodes = 500        # Total episodes for training\n",
    "max_steps = 100             # Max possible steps in an episode\n",
    "batch_size = 64             \n",
    "save_frequency = 100        # Number of episodes before saving checkpoint\n",
    "\n",
    "# Exploration parameters for epsilon greedy strategy\n",
    "explore_start = 1.0            # exploration probability at start\n",
    "explore_stop = 0.01            # minimum exploration probability \n",
    "decay_rate = 0.0001            # exponential decay rate for exploration prob\n",
    "\n",
    "# Q learning hyperparameters\n",
    "gamma = 0.95               # Discounting rate\n",
    "\n",
    "### MEMORY HYPERPARAMETERS\n",
    "pretrain_length = batch_size   # Number of experiences stored in the Memory when initialized for the first time\n",
    "que_length = 1000000          # Number of experiences the Memory can keep\n",
    "\n",
    "training = False\n",
    "trained_model = 'checkpoint400.pth.tar'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createEnvironment():\n",
    "    game = DoomGame()\n",
    "    game.load_config(\"scenarios/basic.cfg\")\n",
    "    game.init()\n",
    "    \n",
    "    left = [1, 0, 0]\n",
    "    right = [0, 1, 0]\n",
    "    shoot = [0, 0, 1]\n",
    "    actions = [left, right, shoot]\n",
    "    \n",
    "    return game, actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processImg(img):\n",
    "#     img = np.swapaxes(img, 0, 2)\n",
    "#     img = color.rgb2gray(img)\n",
    "    img = img[70:-10, :]\n",
    "    img = img / 255\n",
    "    img = transform.resize(img, (200, 200))\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "game, actions = createEnvironment()\n",
    "img = processImg(game.get_state().screen_buffer)\n",
    "img.shape\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_size = 4\n",
    "stacked_frames = deque([np.zeros((200,200), dtype=np.int) for i in range(stack_size)], maxlen=4)\n",
    "def stackFrames(stacked_frames, stateim, is_new_episode):\n",
    "    frame = processImg(stateim)\n",
    "    if is_new_episode:\n",
    "        stacked_frames = deque([np.zeros((200,200), dtype=np.int) for i in range(stack_size)], maxlen=4)\n",
    "        for _ in range(stack_size):\n",
    "            stacked_frames.append(frame)\n",
    "        stacked_state = np.stack(stacked_frames, axis=2)\n",
    "    else:\n",
    "        stacked_frames.append(frame)\n",
    "        stacked_state = np.stack(stacked_frames, axis=2)\n",
    "        \n",
    "    return stacked_state, stacked_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, img_size):\n",
    "        self.img_size = img_size\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(4, 32, kernel_size=8, stride=2, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        self.fc1 = nn.Linear(24*24*128, 512)\n",
    "        self.fc2 = nn.Linear(512, 3)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.elu(self.bn1(self.conv1(x)))\n",
    "        x = F.elu(self.bn2(self.conv2(x)))\n",
    "        x = F.elu(self.bn3(self.conv3(x)))\n",
    "        \n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.elu(self.fc2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNet():\n",
    "    def __init__(self):\n",
    "        self.nnet = DQN(200).cuda()\n",
    "    def train(self, memory, batch_size): \n",
    "        optimizer = optim.Adam(self.nnet.parameters(), lr=learning_rate)\n",
    "        batch = memory.sample(batch_size)\n",
    "        states_mb = torch.FloatTensor(np.array([each[0] for each in batch], ndmin=3)).contiguous().cuda()\n",
    "        actions_mb = torch.FloatTensor(np.array([each[1] for each in batch])).contiguous().cuda()\n",
    "        rewards_mb = torch.FloatTensor(np.array([each[2] for each in batch])).contiguous().cuda()\n",
    "        next_states_mb = torch.FloatTensor(np.array([each[3] for each in batch], ndmin=3).astype(np.float64)).contiguous().cuda()\n",
    "        dones_mb = np.array([each[4] for each in batch])\n",
    "        target_Qs_batch = []\n",
    "        \n",
    "        Qs_next_state = self.nnet(next_states_mb.transpose(1,3))\n",
    "        for i in range(0, len(batch)):\n",
    "            terminal = dones_mb[i]\n",
    "            if terminal:\n",
    "                target_Qs_batch.append(rewards_mb[i])\n",
    "            else:\n",
    "#                 print(Qs_next_state.shape)\n",
    "#                 print(torch.max(Qs_next_state[i], 0))\n",
    "#                 print(torch.max(Qs_next_state[i], 1))\n",
    "                Q_target = rewards_mb[i] + gamma*torch.max(Qs_next_state[i])\n",
    "                target_Qs_batch.append(Q_target)\n",
    "        \n",
    "        targets_mb = torch.FloatTensor(np.array([each for each in target_Qs_batch]).astype(np.float64)).contiguous().cuda()\n",
    "        \n",
    "        states_mb, targets_mb = Variable(states_mb), Variable(targets_mb)\n",
    "        \n",
    "        x = self.nnet(states_mb.transpose(1,3))\n",
    "        loss = self.loss(targets_mb, x, actions_mb)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()   \n",
    "        return loss\n",
    "    \n",
    "    def predict(self, frames):\n",
    "        frames = torch.FloatTensor(frames)\n",
    "        frames = frames.contiguous().cuda().unsqueeze(0)\n",
    "        self.nnet.eval()\n",
    "        x = self.nnet(frames.transpose(1,3))\n",
    "        return x.data.cpu().numpy()[0]\n",
    "    \n",
    "    def save_checkpoint(self, folder='checkpoint', filename='checkpoint.pth.tar'):\n",
    "        filepath = os.path.join(folder, filename)\n",
    "        if not os.path.exists(folder):\n",
    "            print(\"Checkpoint Directory does not exist! Making directory {}\".format(folder))\n",
    "            os.mkdir(folder)\n",
    "        else:\n",
    "            print(\"Saving Checkpoint\")\n",
    "        torch.save({\n",
    "            'state_dict' : self.nnet.state_dict(),\n",
    "        }, filepath)\n",
    "        \n",
    "    def load_checkpoint(self, folder='checkpoint', filename='checkpoint.pth.tar'):\n",
    "    # https://github.com/pytorch/examples/blob/master/imagenet/main.py#L98\n",
    "        filepath = os.path.join(folder, filename)\n",
    "        if not os.path.exists(filepath):\n",
    "            raise(\"No model in path {}\".format(filepath))\n",
    "        checkpoint = torch.load(filepath)\n",
    "        self.nnet.load_state_dict(checkpoint['state_dict'])\n",
    "        \n",
    "    def loss(self, target_Qs, output, actions_):\n",
    "        Qs = torch.sum(output*actions_, dim=1)\n",
    "#         loss = (target_Qs-Qs)**2\n",
    "        return torch.sum((target_Qs-Qs)**2)/target_Qs.size()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory():\n",
    "    def __init__(self, quelength):\n",
    "        self.buffer = deque(maxlen=quelength)\n",
    "        \n",
    "    def add(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        buffer_size = len(self.buffer)\n",
    "        index = np.random.choice(np.arange(buffer_size), batch_size, replace=False)\n",
    "        return [self.buffer[i] for i in index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "game, possible_actions = createEnvironment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = Memory(que_length)\n",
    "game.new_episode()\n",
    "\n",
    "def pretrain():\n",
    "    for i in range(pretrain_length):\n",
    "        if i == 0:\n",
    "            state = game.get_state().screen_buffer\n",
    "            state, stacked_frames = stackFrames(stacked_frames, state, is_new_episode=True)\n",
    "\n",
    "        action = random.choice(possible_actions)\n",
    "        reward = game.make_action(action)\n",
    "        done = game.is_episode_finished()\n",
    "\n",
    "        if done:\n",
    "            next_state = np.zeros(state.shape)\n",
    "            memory.add((state, action, reward, next_state, done))\n",
    "\n",
    "            game.new_episode()\n",
    "            state = game.get_state().screen_buffer\n",
    "            state, stacked_frames = stackFrames(stacked_frames, state, is_new_episode=True)\n",
    "        else:\n",
    "            next_state = game.get_state().screen_buffer\n",
    "            next_state, stacked_frames = stackFrames(stacked_frames, next_state, is_new_episode=False)\n",
    "            memory.add((state, action, reward, next_state, done))\n",
    "            state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnet = NNet()\n",
    "def predict(explore_start, explore_stop, decay_rate, decay_step, state, possible_actions):\n",
    "    exp_exp_tradeoff = np.random.rand()\n",
    "    \n",
    "    explore_probability = explore_stop + (explore_start - explore_stop) * np.exp(-decay_rate * decay_step)\n",
    "    if explore_probability > exp_exp_tradeoff:\n",
    "        action = random.choice(possible_actions)\n",
    "    else:\n",
    "        Qs = nnet.predict(state)\n",
    "        choice = np.argmax(Qs)\n",
    "        action = possible_actions[int(choice)]\n",
    "        \n",
    "    return action, explore_probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def run():\n",
    "    game, actions = createEnvironment()\n",
    "    mem = deque([], maxlen=que_length)\n",
    "    for episode in range(total_episodes):\n",
    "        state = game.get_state()\n",
    "        is_new_episode = True\n",
    "        for step in range(max_steps):\n",
    "            state_im = stackFrames(state.screen_buffer)\n",
    "            \n",
    "            exp_exp_tradeoff = random.uniform(0,1)\n",
    "            if exp_exp_tradeoff > epsilon:\n",
    "                action = nnet.predict(state_im)\n",
    "            else:\n",
    "                action = random.choice(actions)\n",
    "                \n",
    "            reward = game.make_action(action)\n",
    "            new_s = game.get_state()\n",
    "            new_im = rescaleAndGray(new_s.screen_buffer)\n",
    "            \n",
    "            mem.append((state_im, action, reward, new_im))\n",
    "            state = new_s\n",
    "            \n",
    "            Q_target = mem[mem.index(state_im)][2] + gamma*\n",
    "        \n",
    "        episode += 1\n",
    "        epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if training:\n",
    "    pretrain()\n",
    "    decay_step = 0\n",
    "    game.init()\n",
    "    for episode in range(total_episodes):\n",
    "        if episode > 0:\n",
    "            print(f\"Episode: {episode}, Total Reward: {np.sum(episode_rewards)}, Loss: {loss}, Explore: {exploration_probability}\")\n",
    "        step = 0\n",
    "        episode_rewards = []\n",
    "        game.new_episode()\n",
    "        state = game.get_state().screen_buffer\n",
    "        state, stacked_frames = stackFrames(stacked_frames, state, True)\n",
    "        while step < max_steps:\n",
    "            step += 1\n",
    "            decay_step += 1\n",
    "\n",
    "            action, exploration_probability = predict(explore_start, explore_stop, decay_rate, decay_step, state, possible_actions)\n",
    "            reward = game.make_action(action)\n",
    "            done = game.is_episode_finished()\n",
    "            episode_rewards.append(reward)\n",
    "            if done:\n",
    "                next_state = np.zeros((im_size[0], im_size[1]), dtype=np.int)\n",
    "                next_state, stacked_frames = stackFrames(stacked_frames, next_state, False)\n",
    "                step = max_steps\n",
    "                total_reward = np.sum(episode_rewards)\n",
    "    #             print(f\"Episode: {episode}, Total Reward: {total_reward}, Loss: {loss}, Explore: {exploration_probability}\")\n",
    "                memory.add((state, action, reward, next_state, done))\n",
    "            else:\n",
    "                next_state = game.get_state().screen_buffer\n",
    "                next_state, stacked_frames = stackFrames(stacked_frames, next_state, False)\n",
    "                memory.add((state, action, reward, next_state, done))\n",
    "                state = next_state\n",
    "\n",
    "            loss = nnet.train(memory, batch_size=batch_size)\n",
    "        if episode % save_frequency == 0:\n",
    "            nnet.save_checkpoint(filename=f'checkpoint{episode}.pth.tar')\n",
    "    nnet.save_checkpoint(filename='final.pth.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter anything to replay, or nothing to enda\n",
      "Enter anything to replay, or nothing to end\n",
      "Score: 85.0\n"
     ]
    }
   ],
   "source": [
    "nnet.load_checkpoint(filename=trained_model)\n",
    "game.init()\n",
    "\n",
    "done = False\n",
    "game.new_episode()\n",
    "\n",
    "state = game.get_state().screen_buffer\n",
    "state, stacked_frames = stackFrames(stacked_frames, state, True)\n",
    "while not game.is_episode_finished():\n",
    "    Qs = nnet.predict(state)\n",
    "    choice = np.argmax(Qs)\n",
    "    action = possible_actions[int(choice)]\n",
    "    \n",
    "    game.make_action(action)\n",
    "    done = game.is_episode_finished()\n",
    "    score = game.get_total_reward()\n",
    "    \n",
    "    if done:\n",
    "        inp = input('Enter anything to replay, or nothing to end')\n",
    "        if not inp:\n",
    "            break\n",
    "        else:\n",
    "            game.new_episode()\n",
    "            state = game.get_state().screen_buffer\n",
    "            state, stacked_frames = stackFrames(stacked_frames, state, True)\n",
    "    else:\n",
    "        next_state = game.get_state().screen_buffer\n",
    "        next_state, stacked_frames = stackFrames(stacked_frames, next_state, False)\n",
    "        state = next_state\n",
    "    \n",
    "score = game.get_total_reward()\n",
    "print(f'Score: {score}')\n",
    "game.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

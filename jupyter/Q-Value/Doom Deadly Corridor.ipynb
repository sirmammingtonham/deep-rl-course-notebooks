{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import warnings\n",
    "import random\n",
    "import time\n",
    "\n",
    "from vizdoom import *\n",
    "\n",
    "import numpy as np\n",
    "from skimage import transform, util, color\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ethan\\OneDrive\\notebooks\n"
     ]
    }
   ],
   "source": [
    "%cd ..\\..\n",
    "%matplotlib inline\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MODEL HYPERPARAMETERS (from article) ###\n",
    "im_size = [100,100,4]        # 200x200 image x 4 frames per input\n",
    "action_size = 7              # \n",
    "learning_rate =  0.00025      # Alpha (aka learning rate)\n",
    "\n",
    "### TRAINING HYPERPARAMETERS\n",
    "total_episodes = 5000         # Total episodes for training\n",
    "max_steps = 5000              # Max possible steps in an episode\n",
    "batch_size = 64             \n",
    "save_frequency = 1000\n",
    "\n",
    "# FIXED Q TARGETS HYPERPARAMETERS \n",
    "max_tau = 10000 #Tau is the C step where we update our target network\n",
    "\n",
    "# EXPLORATION HYPERPARAMETERS for epsilon greedy strategy\n",
    "explore_start = 1.0            # exploration probability at start\n",
    "explore_stop = 0.01            # minimum exploration probability \n",
    "decay_rate = 0.00005            # exponential decay rate for exploration prob\n",
    "\n",
    "# Q LEARNING hyperparameters\n",
    "gamma = 0.95               # Discounting rate\n",
    "\n",
    "### MEMORY HYPERPARAMETERS\n",
    "## If you have GPU change to 1million\n",
    "# pretrain_length = 100000   # Number of experiences stored in the Memory when initialized for the first time\n",
    "pretrain_length = 10000\n",
    "memory_size = 10000       # Number of experiences the Memory can keep\n",
    "\n",
    "### MODIFY THIS TO FALSE IF YOU JUST WANT TO SEE THE TRAINED AGENT\n",
    "training = False\n",
    "\n",
    "## TURN THIS TO TRUE IF YOU WANT TO RENDER THE ENVIRONMENT\n",
    "episode_render = True\n",
    "trained_model = 'DQNcheckpoint1000.pth.tar'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createEnvironment():\n",
    "    game = DoomGame()\n",
    "    game.load_config(\"scenarios/deadly_corridor.cfg\")\n",
    "    game.init()\n",
    "    \n",
    "    possible_actions = np.identity(7,dtype=int).tolist()\n",
    "    \n",
    "    return game, possible_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processImg(img):\n",
    "#     img = np.swapaxes(img, 0, 2)\n",
    "#     img = color.rgb2gray(img)\n",
    "    img = img[45:-10, :]\n",
    "    img = img / 255\n",
    "    img = transform.resize(img, (im_size[0], im_size[1]))\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "game, actions = createEnvironment()\n",
    "img = processImg(game.get_state().screen_buffer)\n",
    "img.shape\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_size = 3\n",
    "stacked_frames = deque([np.zeros((im_size[0],im_size[1]), dtype=np.int) for i in range(stack_size)], maxlen=3)\n",
    "def stackFrames(stacked_frames, stateim, is_new_episode):\n",
    "    frame = processImg(stateim)\n",
    "    if is_new_episode:\n",
    "        stacked_frames = deque([np.zeros((im_size[0],im_size[1]), dtype=np.int) for i in range(stack_size)], maxlen=3)\n",
    "        for _ in range(stack_size):\n",
    "            stacked_frames.append(frame)\n",
    "        stacked_state = np.stack(stacked_frames, axis=2)\n",
    "    else:\n",
    "        stacked_frames.append(frame)\n",
    "        stacked_state = np.stack(stacked_frames, axis=2)\n",
    "        \n",
    "    return stacked_state, stacked_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class D3QN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=8, stride=2, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        #State value\n",
    "        self.v_fc1 = nn.Linear(12*12*128, 512)\n",
    "        self.v_fc2 = nn.Linear(512, 1)\n",
    "        \n",
    "        #Action\n",
    "        self.a_fc1 = nn.Linear(12*12*128, 512)\n",
    "        self.a_fc2 = nn.Linear(512, 7)\n",
    "    def forward(self, x):\n",
    "        x = F.elu(self.bn1(self.conv1(x)))\n",
    "        x = F.elu(self.bn2(self.conv2(x)))\n",
    "        x = F.elu(self.bn3(self.conv3(x)))\n",
    "        \n",
    "        x = x.view(x.size(0), -1)\n",
    "        v = F.elu(self.v_fc1(x))\n",
    "        v = F.elu(self.v_fc2(v))\n",
    "        \n",
    "        a = F.elu(self.a_fc1(x))\n",
    "        a = F.elu(self.a_fc2(a))\n",
    "        out = v + (a-torch.mean(a, 1, True))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNet():\n",
    "    def __init__(self):\n",
    "        self.nnet = D3QN().cuda()\n",
    "        \n",
    "    def __call__(self):\n",
    "        return self.nnet\n",
    "\n",
    "    def train(self, memory, batch_size, qNetwork): \n",
    "        optimizer = optim.Adam(self.nnet.parameters(), lr=learning_rate)\n",
    "        \n",
    "        tree_idx, batch, ISWeights_mb = memory.sample(batch_size)\n",
    "        \n",
    "        states_mb = torch.FloatTensor([each[0][0] for each in batch]).contiguous().cuda()\n",
    "        actions_mb = torch.FloatTensor([each[0][1] for each in batch]).contiguous().cuda()\n",
    "        rewards_mb = torch.FloatTensor([each[0][2] for each in batch]).contiguous().cuda()\n",
    "        next_states_mb = torch.FloatTensor([each[0][3] for each in batch]).contiguous().cuda()\n",
    "        dones_mb = np.array([each[0][4] for each in batch])\n",
    "        target_Qs_batch = []\n",
    "        \n",
    "        Qs_next_state = qNetwork(next_states_mb.transpose(1,3))\n",
    "        q_target_next_state = self.nnet(next_states_mb.transpose(1,3))\n",
    "        \n",
    "            \n",
    "        for i in range(0, len(batch)):\n",
    "            terminal = dones_mb[i]\n",
    "            action = torch.argmax(Qs_next_state[i])\n",
    "            if terminal:\n",
    "                target_Qs_batch.append(rewards_mb[i])\n",
    "            else:\n",
    "#                 print(Qs_next_state.shape)\n",
    "#                 print(torch.max(Qs_next_state[i], 0))\n",
    "#                 print(torch.max(Qs_next_state[i], 1))\n",
    "                Q_target = rewards_mb[i] + gamma*q_target_next_state[i][action]\n",
    "                target_Qs_batch.append(Q_target)\n",
    "        \n",
    "        targets_mb = torch.FloatTensor([each for each in target_Qs_batch]).contiguous().cuda()\n",
    "        ISWeights_mb = torch.FloatTensor(ISWeights_mb).contiguous().cuda()\n",
    "        \n",
    "        states_mb, targets_mb, ISWeights_mb = Variable(states_mb), Variable(targets_mb), Variable(ISWeights_mb)\n",
    "        \n",
    "        x = self.nnet(states_mb.transpose(1,3))\n",
    "        loss, abs_error = self.loss(targets_mb, x, ISWeights_mb, actions_mb)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        return tree_idx, loss, abs_error\n",
    "    \n",
    "    def predict(self, frames):\n",
    "        frames = torch.FloatTensor(frames)\n",
    "        frames = frames.contiguous().cuda().unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            frames = Variable(frames)\n",
    "        self.nnet.eval()\n",
    "        x = self.nnet(frames.transpose(1,3))\n",
    "        return x.data.cpu().numpy()[0]\n",
    "    \n",
    "    def load_weights(self, Nnet):\n",
    "        self.nnet.load_state_dict(Nnet.state_dict())\n",
    "    \n",
    "    def loss(self, target_Qs, output, ISWeights_, actions_):\n",
    "        Qs = torch.sum(output*actions_, dim=1)\n",
    "        with torch.no_grad():\n",
    "            absolute_errors = torch.abs(target_Qs-Qs)\n",
    "#         loss = (target_Qs-Qs)**2\n",
    "        return torch.mean(ISWeights_ * (target_Qs-Qs)**2), absolute_errors\n",
    "\n",
    "    def save_checkpoint(self, folder='checkpoint', filename='checkpoint.pth.tar'):\n",
    "        filepath = os.path.join(folder, filename)\n",
    "        if not os.path.exists(folder):\n",
    "            print(\"Checkpoint Directory does not exist! Making directory {}\".format(folder))\n",
    "            os.mkdir(folder)\n",
    "        else:\n",
    "            print(\"Saving Checkpoint\")\n",
    "        torch.save({\n",
    "            'state_dict' : self.nnet.state_dict(),\n",
    "        }, filepath)\n",
    "        \n",
    "    def load_checkpoint(self, folder='checkpoint', filename='checkpoint.pth.tar'):\n",
    "    # https://github.com/pytorch/examples/blob/master/imagenet/main.py#L98\n",
    "        filepath = os.path.join(folder, filename)\n",
    "        if not os.path.exists(filepath):\n",
    "            raise(\"No model in path {}\".format(filepath))\n",
    "        checkpoint = torch.load(filepath)\n",
    "        self.nnet.load_state_dict(checkpoint['state_dict'])\n",
    "        \n",
    "    @property\n",
    "    def state_dict(self):\n",
    "        return self.nnet.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "DQNetwork = NNet()\n",
    "TargetNetwork = NNet()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class SumTree():\n",
    "    data_pointer = 0\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.tree = np.zeros(2 * capacity - 1)\n",
    "        self.data = np.zeros(capacity, dtype=object)\n",
    "        \n",
    "    def add(self, priority, data):\n",
    "        tree_index = self.data_pointer + self.capacity - 1\n",
    "        \n",
    "        self.data[self.data_pointer] = data\n",
    "        self.update(tree_index, priority)\n",
    "        self.data_pointer += 1\n",
    "        if self.data_pointer >= self.capacity:\n",
    "            self.data_pointer = 0\n",
    "            \n",
    "    def update(self, tree_index, priority):\n",
    "        change = priority - self.tree[tree_index]\n",
    "        self.tree[tree_index] = priority\n",
    "        \n",
    "        while tree_index != 0:\n",
    "            tree_index = (tree_index-1) // 2\n",
    "            self.tree[tree_index] += change\n",
    "    def get_leaf(self, v):\n",
    "        parent_index = 0\n",
    "        while True:\n",
    "            left_child_index = 2 * parent_index + 1\n",
    "            right_child_index = left_child_index + 1\n",
    "            \n",
    "            if left_child_index >= len(self.tree):\n",
    "                leaf_index = parent_index\n",
    "                break\n",
    "                \n",
    "            else:\n",
    "                if v <= self.tree[left_child_index]:\n",
    "                    parent_index = left_child_index\n",
    "                    \n",
    "                else:\n",
    "                    v -= self.tree[left_child_index]\n",
    "                    parent_index = right_child_index\n",
    "                    \n",
    "        data_index = leaf_index - self.capacity - 1\n",
    "        return leaf_index, self.tree[leaf_index], self.data[data_index]\n",
    "    \n",
    "    @property\n",
    "    def total_priority(self):\n",
    "        return self.tree[0] #return root node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class Memory():\n",
    "    PAR_e = 0.01\n",
    "    PAR_a = 0.6\n",
    "    PAR_b = 0.4\n",
    "    PAR_b_increment = 0.001\n",
    "    abs_error_clip = 1.\n",
    "    \n",
    "    def __init__(self, capacity):\n",
    "        self.tree = SumTree(capacity)\n",
    "    \n",
    "    def add(self, experience):\n",
    "        max_priority = np.max(self.tree.tree[-self.tree.capacity:])\n",
    "        if max_priority == 0:\n",
    "            max_priority = self.abs_error_clip\n",
    "        self.tree.add(max_priority, experience)\n",
    "    \n",
    "    def sample(self, n):\n",
    "        memory_b = []\n",
    "        \n",
    "        b_idx, b_ISWeights = np.empty((n,), dtype=np.int32), np.empty((n, 1), dtype=np.float32)\n",
    "        \n",
    "        priority_segment = self.tree.total_priority / n #divide priority into ranges\n",
    "        self.PAR_b = np.min([1., self.PAR_b + self.PAR_b_increment]) #increasing b each sampling\n",
    "        \n",
    "        p_min = np.min(self.tree.tree[-self.tree.capacity:]) / self.tree.total_priority\n",
    "        max_weight = (p_min * n) ** (-self.PAR_b) #(1/n * 1/p)^b\n",
    "        \n",
    "        for i in range(n):\n",
    "            a, b = priority_segment * i, priority_segment * (i + 1)\n",
    "            value = np.random.uniform(a, b)\n",
    "            \n",
    "            index, priority, data = self.tree.get_leaf(value)\n",
    "            \n",
    "            sampling_probabilities = priority / self.tree.total_priority #P(j)\n",
    "            \n",
    "            #IS = (1/N * 1/P(i))^b /max wi == (N*P(i))**-b  /max wi\n",
    "            b_ISWeights[i, 0] = np.power(n * sampling_probabilities, -self.PAR_b)/ max_weight\n",
    "            b_idx[i] = index\n",
    "            experience = [data]\n",
    "            memory_b.append(experience)\n",
    "            \n",
    "        return b_idx, memory_b, b_ISWeights\n",
    "    \n",
    "    def batch_update(self, tree_idx, abs_errors):\n",
    "        abs_errors += self.PAR_e  # convert to abs and avoid 0\n",
    "        clipped_errors = np.minimum(abs_errors, self.abs_error_clip)\n",
    "        ps = np.power(clipped_errors, self.PAR_a)\n",
    "\n",
    "        for ti, p in zip(tree_idx, ps):\n",
    "            self.tree.update(ti, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SumTree(object):\n",
    "    \"\"\"\n",
    "    This SumTree code is modified version of Morvan Zhou: \n",
    "    https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow/blob/master/contents/5.2_Prioritized_Replay_DQN/RL_brain.py\n",
    "    \"\"\"\n",
    "    data_pointer = 0\n",
    "    \n",
    "    \"\"\"\n",
    "    Here we initialize the tree with all nodes = 0, and initialize the data with all values = 0\n",
    "    \"\"\"\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity # Number of leaf nodes (final nodes) that contains experiences\n",
    "        \n",
    "        # Generate the tree with all nodes values = 0\n",
    "        # To understand this calculation (2 * capacity - 1) look at the schema above\n",
    "        # Remember we are in a binary node (each node has max 2 children) so 2x size of leaf (capacity) - 1 (root node)\n",
    "        # Parent nodes = capacity - 1\n",
    "        # Leaf nodes = capacity\n",
    "        self.tree = np.zeros(2 * capacity - 1)\n",
    "        \n",
    "        \"\"\" tree:\n",
    "            0\n",
    "           / \\\n",
    "          0   0\n",
    "         / \\ / \\\n",
    "        0  0 0  0  [Size: capacity] it's at this line that there is the priorities score (aka pi)\n",
    "        \"\"\"\n",
    "        \n",
    "        # Contains the experiences (so the size of data is capacity)\n",
    "        self.data = np.zeros(capacity, dtype=object)\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    Here we add our priority score in the sumtree leaf and add the experience in data\n",
    "    \"\"\"\n",
    "    def add(self, priority, data):\n",
    "        # Look at what index we want to put the experience\n",
    "        tree_index = self.data_pointer + self.capacity - 1\n",
    "        \n",
    "        \"\"\" tree:\n",
    "            0\n",
    "           / \\\n",
    "          0   0\n",
    "         / \\ / \\\n",
    "tree_index  0 0  0  We fill the leaves from left to right\n",
    "        \"\"\"\n",
    "        \n",
    "        # Update data frame\n",
    "        self.data[self.data_pointer] = data\n",
    "        \n",
    "        # Update the leaf\n",
    "        self.update (tree_index, priority)\n",
    "        \n",
    "        # Add 1 to data_pointer\n",
    "        self.data_pointer += 1\n",
    "        \n",
    "        if self.data_pointer >= self.capacity:  # If we're above the capacity, you go back to first index (we overwrite)\n",
    "            self.data_pointer = 0\n",
    "            \n",
    "    \n",
    "    \"\"\"\n",
    "    Update the leaf priority score and propagate the change through tree\n",
    "    \"\"\"\n",
    "    def update(self, tree_index, priority):\n",
    "        # Change = new priority score - former priority score\n",
    "        change = priority - self.tree[tree_index]\n",
    "        self.tree[tree_index] = priority\n",
    "        \n",
    "        # then propagate the change through tree\n",
    "        while tree_index != 0:    # this method is faster than the recursive loop in the reference code\n",
    "            \n",
    "            \"\"\"\n",
    "            Here we want to access the line above\n",
    "            THE NUMBERS IN THIS TREE ARE THE INDEXES NOT THE PRIORITY VALUES\n",
    "            \n",
    "                0\n",
    "               / \\\n",
    "              1   2\n",
    "             / \\ / \\\n",
    "            3  4 5  [6] \n",
    "            \n",
    "            If we are in leaf at index 6, we updated the priority score\n",
    "            We need then to update index 2 node\n",
    "            So tree_index = (tree_index - 1) // 2\n",
    "            tree_index = (6-1)//2\n",
    "            tree_index = 2 (because // round the result)\n",
    "            \"\"\"\n",
    "            tree_index = (tree_index - 1) // 2\n",
    "            self.tree[tree_index] += change\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    Here we get the leaf_index, priority value of that leaf and experience associated with that index\n",
    "    \"\"\"\n",
    "    def get_leaf(self, v):\n",
    "        \"\"\"\n",
    "        Tree structure and array storage:\n",
    "        Tree index:\n",
    "             0         -> storing priority sum\n",
    "            / \\\n",
    "          1     2\n",
    "         / \\   / \\\n",
    "        3   4 5   6    -> storing priority for experiences\n",
    "        Array type for storing:\n",
    "        [0,1,2,3,4,5,6]\n",
    "        \"\"\"\n",
    "        parent_index = 0\n",
    "        \n",
    "        while True: # the while loop is faster than the method in the reference code\n",
    "            left_child_index = 2 * parent_index + 1\n",
    "            right_child_index = left_child_index + 1\n",
    "            \n",
    "            # If we reach bottom, end the search\n",
    "            if left_child_index >= len(self.tree):\n",
    "                leaf_index = parent_index\n",
    "                break\n",
    "            \n",
    "            else: # downward search, always search for a higher priority node\n",
    "                \n",
    "                if v <= self.tree[left_child_index]:\n",
    "                    parent_index = left_child_index\n",
    "                    \n",
    "                else:\n",
    "                    v -= self.tree[left_child_index]\n",
    "                    parent_index = right_child_index\n",
    "            \n",
    "        data_index = leaf_index - self.capacity + 1\n",
    "\n",
    "        return leaf_index, self.tree[leaf_index], self.data[data_index]\n",
    "    \n",
    "    @property\n",
    "    def total_priority(self):\n",
    "        return self.tree[0] # Returns the root node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory(object):  # stored as ( s, a, r, s_ ) in SumTree\n",
    "    \"\"\"\n",
    "    This SumTree code is modified version and the original code is from:\n",
    "    https://github.com/jaara/AI-blog/blob/master/Seaquest-DDQN-PER.py\n",
    "    \"\"\"\n",
    "    PER_e = 0.01  # Hyperparameter that we use to avoid some experiences to have 0 probability of being taken\n",
    "    PER_a = 0.6  # Hyperparameter that we use to make a tradeoff between taking only exp with high priority and sampling randomly\n",
    "    PER_b = 0.4  # importance-sampling, from initial value increasing to 1\n",
    "    \n",
    "    PER_b_increment_per_sampling = 0.001\n",
    "    \n",
    "    absolute_error_upper = 1.  # clipped abs error\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        # Making the tree \n",
    "        \"\"\"\n",
    "        Remember that our tree is composed of a sum tree that contains the priority scores at his leaf\n",
    "        And also a data array\n",
    "        We don't use deque because it means that at each timestep our experiences change index by one.\n",
    "        We prefer to use a simple array and to overwrite when the memory is full.\n",
    "        \"\"\"\n",
    "        self.tree = SumTree(capacity)\n",
    "        \n",
    "    \"\"\"\n",
    "    Store a new experience in our tree\n",
    "    Each new experience have a score of max_prority (it will be then improved when we use this exp to train our DDQN)\n",
    "    \"\"\"\n",
    "    def add(self, experience):\n",
    "        # Find the max priority\n",
    "        max_priority = np.max(self.tree.tree[-self.tree.capacity:])\n",
    "        \n",
    "        # If the max priority = 0 we can't put priority = 0 since this exp will never have a chance to be selected\n",
    "        # So we use a minimum priority\n",
    "        if max_priority == 0:\n",
    "            max_priority = self.absolute_error_upper\n",
    "        \n",
    "        self.tree.add(max_priority, experience)   # set the max p for new p\n",
    "\n",
    "        \n",
    "    \"\"\"\n",
    "    - First, to sample a minibatch of k size, the range [0, priority_total] is / into k ranges.\n",
    "    - Then a value is uniformly sampled from each range\n",
    "    - We search in the sumtree, the experience where priority score correspond to sample values are retrieved from.\n",
    "    - Then, we calculate IS weights for each minibatch element\n",
    "    \"\"\"\n",
    "    def sample(self, n):\n",
    "        # Create a sample array that will contains the minibatch\n",
    "        memory_b = []\n",
    "        \n",
    "        b_idx, b_ISWeights = np.empty((n,), dtype=np.int32), np.empty((n, 1), dtype=np.float32)\n",
    "        \n",
    "        # Calculate the priority segment\n",
    "        # Here, as explained in the paper, we divide the Range[0, ptotal] into n ranges\n",
    "        priority_segment = self.tree.total_priority / n       # priority segment\n",
    "    \n",
    "        # Here we increasing the PER_b each time we sample a new minibatch\n",
    "        self.PER_b = np.min([1., self.PER_b + self.PER_b_increment_per_sampling])  # max = 1\n",
    "        \n",
    "        # Calculating the max_weight\n",
    "        p_min = np.min(self.tree.tree[-self.tree.capacity:]) / self.tree.total_priority\n",
    "        max_weight = (p_min * n) ** (-self.PER_b)\n",
    "        \n",
    "        for i in range(n):\n",
    "            \"\"\"\n",
    "            A value is uniformly sample from each range\n",
    "            \"\"\"\n",
    "            a, b = priority_segment * i, priority_segment * (i + 1)\n",
    "            value = np.random.uniform(a, b)\n",
    "            \n",
    "            \"\"\"\n",
    "            Experience that correspond to each value is retrieved\n",
    "            \"\"\"\n",
    "            index, priority, data = self.tree.get_leaf(value)\n",
    "            \n",
    "            #P(j)\n",
    "            sampling_probabilities = priority / self.tree.total_priority\n",
    "            \n",
    "            #  IS = (1/N * 1/P(i))**b /max wi == (N*P(i))**-b  /max wi\n",
    "            b_ISWeights[i, 0] = np.power(n * sampling_probabilities, -self.PER_b)/ max_weight\n",
    "                                   \n",
    "            b_idx[i]= index\n",
    "            \n",
    "            experience = [data]\n",
    "            \n",
    "            memory_b.append(experience)\n",
    "        \n",
    "        return b_idx, memory_b, b_ISWeights\n",
    "    \n",
    "    \"\"\"\n",
    "    Update the priorities on the tree\n",
    "    \"\"\"\n",
    "    def batch_update(self, tree_idx, abs_errors):\n",
    "        abs_errors += self.PER_e  # convert to abs and avoid 0\n",
    "        clipped_errors = np.minimum(abs_errors, self.absolute_error_upper)\n",
    "        ps = np.power(clipped_errors, self.PER_a)\n",
    "\n",
    "        for ti, p in zip(tree_idx, ps):\n",
    "            self.tree.update(ti, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "game, possible_actions = createEnvironment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = Memory(memory_size)\n",
    "game.new_episode()\n",
    "\n",
    "if training:\n",
    "    for i in range(pretrain_length):\n",
    "        if i == 0:\n",
    "            state = game.get_state().screen_buffer\n",
    "            state, stacked_frames = stackFrames(stacked_frames, state, is_new_episode=True)\n",
    "\n",
    "        action = random.choice(possible_actions)\n",
    "        reward = game.make_action(action)\n",
    "        done = game.is_episode_finished()\n",
    "\n",
    "        if done:\n",
    "            next_state = np.zeros(state.shape)\n",
    "            memory.add((state, action, reward, next_state, done))\n",
    "\n",
    "            game.new_episode()\n",
    "            state = game.get_state().screen_buffer\n",
    "            state, stacked_frames = stackFrames(stacked_frames, state, is_new_episode=True)\n",
    "        else:\n",
    "            next_state = game.get_state().screen_buffer\n",
    "            next_state, stacked_frames = stackFrames(stacked_frames, next_state, is_new_episode=False)\n",
    "            memory.add((state, action, reward, next_state, done))\n",
    "            state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(explore_start, explore_stop, decay_rate, decay_step, state, possible_actions):\n",
    "    exp_exp_tradeoff = np.random.rand()\n",
    "    \n",
    "    explore_probability = explore_stop + (explore_start - explore_stop) * np.exp(-decay_rate * decay_step)\n",
    "    if explore_probability > exp_exp_tradeoff:\n",
    "        action = random.choice(possible_actions)\n",
    "    else:\n",
    "        Qs = DQNetwork.predict(state)\n",
    "        choice = np.argmax(Qs)\n",
    "        action = possible_actions[int(choice)]\n",
    "        \n",
    "    return action, explore_probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if training:\n",
    "    decay_step = 0\n",
    "    tau = 0\n",
    "    \n",
    "    game.init()\n",
    "    TargetNetwork.load_weights(DQNetwork())\n",
    "    \n",
    "    for episode in range(total_episodes):\n",
    "        if episode > 0:\n",
    "            print(f\"Episode: {episode}, Total Reward: {np.sum(episode_rewards)}, Loss: {loss}, Explore: {exploration_probability}\")\n",
    "        step = 0\n",
    "        episode_rewards = []\n",
    "        game.new_episode()\n",
    "        state = game.get_state().screen_buffer\n",
    "        state, stacked_frames = stackFrames(stacked_frames, state, True)\n",
    "        while step < max_steps:\n",
    "            step += 1\n",
    "            tau += 1\n",
    "            decay_step += 1\n",
    "\n",
    "            action, exploration_probability = predict(explore_start, explore_stop, decay_rate, decay_step, state, possible_actions)\n",
    "            reward = game.make_action(action)\n",
    "            done = game.is_episode_finished()\n",
    "            episode_rewards.append(reward)\n",
    "            if done:\n",
    "                next_state = np.zeros((im_size[0], im_size[1]), dtype=np.int)\n",
    "                next_state, stacked_frames = stackFrames(stacked_frames, next_state, False)\n",
    "                step = max_steps\n",
    "                total_reward = np.sum(episode_rewards)\n",
    "    #             print(f\"Episode: {episode}, Total Reward: {total_reward}, Loss: {loss}, Explore: {exploration_probability}\")\n",
    "                memory.add((state, action, reward, next_state, done))\n",
    "            else:\n",
    "                next_state = game.get_state().screen_buffer\n",
    "                next_state, stacked_frames = stackFrames(stacked_frames, next_state, False)\n",
    "                memory.add((state, action, reward, next_state, done))\n",
    "                state = next_state\n",
    "\n",
    "            tree_idx, loss, abs_errors = TargetNetwork.train(memory, batch_size, DQNetwork())\n",
    "            memory.batch_update(tree_idx, abs_errors)\n",
    "            \n",
    "            if tau > max_tau:\n",
    "                TargetNetwork.load_weights(DQNetwork())\n",
    "                tau = 0\n",
    "                print(\"Model updated\")\n",
    "            \n",
    "        if episode % save_frequency == 0:\n",
    "            TargetNetwork.save_checkpoint(filename=f'Targetscheckpoint{episode}.pth.tar')\n",
    "            DQNetwork.save_checkpoint(filename=f'DQNcheckpoint{episode}.pth.tar')\n",
    "    TargetNetwork.save_checkpoint(filename='final.pth.tar')\n",
    "    DQNetwork.save_checkpoint(filename=f'DQNcheckpoint{episode}.pth.tar')\n",
    "    game.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: -115.99295043945312\n",
      "Score: -115.99655151367188\n",
      "Score: -115.99655151367188\n",
      "Score: -115.99655151367188\n",
      "Score: -115.99246215820312\n",
      "Score: -115.99053955078125\n",
      "Score: -115.99655151367188\n",
      "Score: -115.58798217773438\n",
      "Score: -115.99165344238281\n",
      "Score: -115.25718688964844\n",
      "Score: -115.99655151367188\n",
      "Score: -115.99655151367188\n",
      "Score: -115.99655151367188\n",
      "Score: -115.99655151367188\n",
      "Score: -115.99655151367188\n",
      "Score: -115.99655151367188\n",
      "Score: -115.99655151367188\n"
     ]
    }
   ],
   "source": [
    "if not training:\n",
    "    DQNetwork.load_checkpoint(filename=trained_model)\n",
    "    game.init()\n",
    "\n",
    "    done = False\n",
    "    \n",
    "    for i in range(10000):\n",
    "        game.new_episode()\n",
    "        state = game.get_state().screen_buffer\n",
    "        state, stacked_frames = stackFrames(stacked_frames, state, True)\n",
    "        while not game.is_episode_finished():\n",
    "            Qs = DQNetwork.predict(state)\n",
    "            choice = np.argmax(Qs)\n",
    "            action = possible_actions[int(choice)]\n",
    "\n",
    "            game.make_action(action)\n",
    "            done = game.is_episode_finished()\n",
    "            score = game.get_total_reward()\n",
    "\n",
    "            if done:\n",
    "        #         inp = input('Enter anything to replay, or nothing to end')\n",
    "        #         if not inp:\n",
    "                break\n",
    "    #             else:\n",
    "    #                 game.new_episode()\n",
    "    #                 state = game.get_state().screen_buffer\n",
    "    #                 state, stacked_frames = stackFrames(stacked_frames, state, True)\n",
    "            else:\n",
    "                next_state = game.get_state().screen_buffer\n",
    "                next_state, stacked_frames = stackFrames(stacked_frames, next_state, False)\n",
    "                state = next_state\n",
    "\n",
    "        score = game.get_total_reward()\n",
    "        print(f'Score: {score}')\n",
    "    game.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
